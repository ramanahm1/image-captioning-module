{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b938ac54-798f-40f9-b185-4c9aac1c0fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7c8d56-4756-44f1-a056-6500d82dd354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\n",
      "  Using cached scikit_image-0.23.2-cp312-cp312-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-image) (1.26.4)\n",
      "Collecting scipy>=1.9 (from scikit-image)\n",
      "  Using cached scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: networkx>=2.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-image) (3.3)\n",
      "Requirement already satisfied: pillow>=9.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-image) (10.3.0)\n",
      "Collecting imageio>=2.33 (from scikit-image)\n",
      "  Using cached imageio-2.34.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image)\n",
      "  Using cached tifffile-2024.5.22-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: packaging>=21 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-image) (24.0)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Using cached scikit_image-0.23.2-cp312-cp312-macosx_12_0_arm64.whl (13.3 MB)\n",
      "Using cached imageio-2.34.1-py3-none-any.whl (313 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl (30.4 MB)\n",
      "Using cached tifffile-2024.5.22-py3-none-any.whl (225 kB)\n",
      "Installing collected packages: tifffile, scipy, lazy-loader, imageio, scikit-image\n",
      "Successfully installed imageio-2.34.1 lazy-loader-0.4 scikit-image-0.23.2 scipy-1.13.1 tifffile-2024.5.22\n"
     ]
    }
   ],
   "source": [
    "!pip3 install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdc79e88-51b5-4e9f-99a3-d30a32b99859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import os\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as nnf\n",
    "import sys\n",
    "from typing import Tuple, List, Union, Optional\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import skimage.io as io\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a03a515-13e7-420b-9b1c-439995e67853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Features:\n",
      "<class 'torch.Tensor'>\n",
      "Img features dim: torch.Size([512])\n",
      "image_features dimensions: torch.Size([512])\n",
      "input_ids dimensions: torch.Size([13])\n",
      "<class 'torch.Tensor'>\n",
      "Ip vals max: tensor(5836)\n",
      "Ip vals min: tensor(25)\n",
      "<class 'torch.Tensor'>\n",
      "image feature max: tensor(1)\n",
      "image feature min: tensor(-6)\n",
      "Vocabulary Size: 50257\n",
      "Vocabulary Size: 50257\n",
      "pacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespaces!pacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespaces 313pacespacespacespacespacespacespacespacespaces 313pacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespaces 313pacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacespacesQ: What's happening in the image? one word answer: a few words Q:\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load CLIP model and processor (replace with your chosen model URL/name)\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load text generation model and tokenizer (replace with your chosen model)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model_lm = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def generate_caption(image_path, max_length=5, temperature=1.0):\n",
    "  # Preprocess the image\n",
    "  image = Image.open(image_path)\n",
    "  inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "  vals = inputs['pixel_values']\n",
    "  # Encode the image with CLIP\n",
    "  with torch.no_grad():\n",
    "      image_features = model.get_image_features(vals).unsqueeze(0).long()\n",
    "      image_features = image_features.reshape(image_features.shape[2],)\n",
    "  print(\"Image Features:\")\n",
    "  print(type(image_features))\n",
    "\n",
    "  print(\"Img features dim:\", image_features.shape)\n",
    "\n",
    "#### Done with image preprocessing we have the features as a tensor for shape (512,).\n",
    "    \n",
    "  # Generate a prompt based on image features (replace with your logic)\n",
    "  prompt = \"Q: What's happening in the image? one word answer:\"  # Placeholder for inferred content\n",
    "\n",
    "  # Encode the prompt\n",
    "  input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].unsqueeze(0).long()\n",
    "  input_ids = input_ids.reshape(input_ids.shape[-1],)\n",
    "\n",
    "  print(\"image_features dimensions:\", image_features.shape)\n",
    "  print(\"input_ids dimensions:\", input_ids.shape)\n",
    "\n",
    "  print(type(input_ids))\n",
    "  print(\"Ip vals max:\", input_ids.max())\n",
    "  print(\"Ip vals min:\", input_ids.min())\n",
    "\n",
    "  print(type(image_features))\n",
    "  print(\"image feature max:\", image_features.max())\n",
    "  print(\"image feature min:\", image_features.min())\n",
    "\n",
    "  # Debugging:\n",
    "  vocab_size = tokenizer.vocab_size\n",
    "  print(f\"Vocabulary Size: {vocab_size}\")\n",
    "\n",
    "  # Normalize image features to the range [0, 1]\n",
    "  image_features = (image_features - image_features.min()) / (image_features.max() - image_features.min())\n",
    "    \n",
    "    # Scale normalized features to the range [0, vocab_size - 1]\n",
    "  image_features = (image_features * (tokenizer.vocab_size - 1))\n",
    "\n",
    "\n",
    "  # Concatenate image and text embeddings (optional, experiment with different approaches)\n",
    "  combined_input = torch.cat((image_features.unsqueeze(0).long(), input_ids.unsqueeze(0).long()), dim=-1)\n",
    "\n",
    "\n",
    "  # Concatenation was successful\n",
    "  output = model_lm.generate(\n",
    "      combined_input,\n",
    "      do_sample=True,\n",
    "      max_new_tokens=max_length,\n",
    "      temperature=temperature,\n",
    "      top_k=20,\n",
    "      top_p=0.9,\n",
    "      no_repeat_ngram_size=3,\n",
    "  )\n",
    "\n",
    "  vocab_size = tokenizer.vocab_size\n",
    "  print(f\"Vocabulary Size: {vocab_size}\")\n",
    "\n",
    "    \n",
    "    # Decode the generated text\n",
    "  caption = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "  return caption\n",
    "\n",
    "# Example usage\n",
    "image_path = \"running.jpeg\"\n",
    "caption = generate_caption(image_path)\n",
    "print(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ca37c-f4ad-4e10-b76f-a779fa226c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4e3179-5412-400d-b16c-cc92efdbfe02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
